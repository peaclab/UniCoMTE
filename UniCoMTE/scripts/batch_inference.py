'''
    Purpose: run batch inference
'''

# make imports
import os
import h5py
import math
import pandas as pd
import numpy as np
import time
# project modules
from tensorflow.keras.utils import Sequence
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,
                                        CSVLogger, EarlyStopping)
from tensorflow.keras.models import load_model
from sklearn.pipeline import Pipeline
# Project modules
# import sys
# sys.path.insert(0,"/projectnb/peaclab-mon/JLi/projectx/CoMTE_V2_JLi/datasets")
# import datasets as datasets

# define environmental variables to prevent overuse of CPU Cores
# Access and modify environmental variables
os.environ['TF_NUM_INTRAOP_THREADS'] = '1' #set to 1
os.environ['TF_NUM_INTEROP_THREADS'] = '3' #set to 1 less than # of requested cores
print(f"TF_NUM_INTRAOP_THREADS is {os.getenv('TF_NUM_INTRAOP_THREADS')}")
print(f"TF_NUM_INTEROP_THREADS is {os.getenv('TF_NUM_INTEROP_THREADS')}")

# Define functions
def batch_inference(data, model, batch_size = 500):
    
    results = []
    
    for i in range(0,data.shape[0], batch_size):
        print(f'working on indicies {i} to {i+batch_size}')
        batch = data[i:i+batch_size]
        batch_results = model.predict(batch, verbose = 1)
        results.append(batch_results)
    
    print(len(results))
    
    return np.concatenate(results)


# load in full CODE15% data
code15_all_tracings_path = "/projectnb/peaclab-mon/JLi/projectx/CoMTE_V2_JLi/all_parts_hdf_tracings.npy"
tracings = np.load(code15_all_tracings_path, allow_pickle=True)  # allow_pickle=True if objects are stored
print(f"shape of tracings: {tracings.shape}")

# load pretrained model 
model_path = "/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/PretrainedModels/model/model.hdf5"
pre_model = load_model(model_path)  
pre_model.compile(loss='binary_crossentropy', optimizer=Adam())

# run inference with batch inference function
x_subset = tracings
model_predictions = batch_inference(x_subset, pre_model)   # y_score is a numpy array with dimensions 827x6. It holds the predictions generated by the model

# extra
print('batch inference completed')
print(f'shape of predictions: {model_predictions.shape}')

# Generate dataframe
np.save("/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/full_trainingset_outputs.npy", model_predictions)
print("Output predictions saved")
