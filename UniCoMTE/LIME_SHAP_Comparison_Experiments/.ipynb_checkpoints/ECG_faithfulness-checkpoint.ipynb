{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are the explanations faithful to the original classifier?\n",
    "\n",
    "* Use HPC Time series data\n",
    "* Train a “golden classifier”, e.g., sparse logistic regression\n",
    "* Only a few metrics are used for classification\n",
    "* Get explanations for each sample in the test set\n",
    "* Report precision/recall for explanations\n",
    "\n",
    "\n",
    "\n",
    "- To test the faithfulness of CoMTE, we explain a simple model with a known reasoning process and report the precision and recall of our explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "import functools\n",
    "import sys \n",
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "import mlrose_ky\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loading\n",
    "import analysis.data\n",
    "import analysis.classifier\n",
    "import explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.DEBUG)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(pipeline, threshold=0.1):\n",
    "    return set([\n",
    "        '_'.join(x.split('_')[1:]) \n",
    "        for idx, x in enumerate(pipeline.steps[2][1].column_names) \n",
    "        if idx in np.where(np.abs(pipeline.steps[4][1].coef_) > threshold)[1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.91it/s]\n",
      "100%|██████████| 226/226 [00:16<00:00, 14.00it/s]\n"
     ]
    }
   ],
   "source": [
    "timeseries, labels, test_timeseries, test_labels = data_loading.load_hpc_data(\n",
    "#    Path('/home/ates/data/taxonomist/'), window=60, skip=60, make_binary=True)\n",
    "    # Path('/projectnb/peaclab-mon/ates/taxonomist/'), window=60, skip=60, make_binary=True)\n",
    "    Path('/projectnb/peaclab-mon/ates/hpas'), classes=['none', 'dcopy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5c1316901963fd7bbf0f90ab_45\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "for idx, row in test_labels.iterrows():\n",
    "    print(idx[0])\n",
    "    print(row['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = analysis.data.TSFeatureGenerator(trim=0)\n",
    "pipeline = Pipeline([\n",
    "    ('assert1', analysis.classifier.CheckFeatures()),\n",
    "    ('features', analysis.data.TSFeatureGenerator(trim=0)),\n",
    "    ('assert2', analysis.classifier.CheckFeatures()),\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1))),\n",
    "    #('clf', RandomForestClassifier(n_estimators=100, class_weight='balanced'))\n",
    "    ('clf', LogisticRegression(penalty='l1', C=0.1, solver='liblinear'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.classes_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/projectnb/peaclab-mon/JLi/projectx/Past_Work/SandiaDataAnalysis/explainability/analysis/data.py\", line 495, in __call__\n    return _get_features(\n  File \"/projectnb/peaclab-mon/JLi/projectx/Past_Work/SandiaDataAnalysis/explainability/analysis/data.py\", line 473, in _get_features\n    generate_features(data, trim).reshape((1, len(_TIMESERIES.columns) * 11)),\nNameError: name 'generate_features' is not defined. Did you mean: 'get_features'?\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m C \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mset_params(clf__C\u001b[38;5;241m=\u001b[39mC)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     all_metrics \u001b[38;5;241m=\u001b[39m get_metrics(pipeline)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_metrics) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m     )\n\u001b[1;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    586\u001b[0m )\n\u001b[0;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1555\u001b[0m         )\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/peaclab-mon/JustinECG_P10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/projectnb/peaclab-mon/JLi/projectx/Past_Work/SandiaDataAnalysis/explainability/analysis/data.py:537\u001b[0m, in \u001b[0;36mTSFeatureGenerator.transform\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    534\u001b[0m     result \u001b[38;5;241m=\u001b[39m [extractor(node_id)\n\u001b[1;32m    535\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(index_name)\u001b[38;5;241m.\u001b[39munique()]\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    541\u001b[0m     pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/share/pkg.8/python3/3.10.12/install/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/python3/3.10.12/install/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_features' is not defined"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "\n",
    "for C in np.logspace(2, -5, num=10):\n",
    "    pipeline.set_params(clf__C=C)\n",
    "    pipeline.fit(timeseries, labels)\n",
    "    all_metrics = get_metrics(pipeline)\n",
    "    if len(all_metrics) < 10:\n",
    "        break\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics(pipeline)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.predict(test_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score:\", f1_score(test_labels, preds, average='weighted'))\n",
    "for label, i in zip(test_labels['label'].unique(), f1_score(test_labels, preds, labels=test_labels['label'].unique(), average=None)):\n",
    "    print(\"\\t\", label, i)\n",
    "\n",
    "label_list = pipeline.steps[4][1].classes_\n",
    "cf = confusion_matrix(test_labels, preds, labels=label_list).astype(float)\n",
    "for i in range(len(cf)):\n",
    "    cf[i] = [x / cf[i].sum() for x in cf[i]]\n",
    "sns.heatmap(cf, annot=True, xticklabels=label_list, yticklabels=label_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive \"healthy\"\n",
    "#negative \"other\"\n",
    "positive = np.unique(test_labels)[0]\n",
    "negative = np.unique(test_labels)[1]\n",
    "\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "\n",
    "for pred, (idx, row) in zip(preds, test_labels.iterrows()):    \n",
    "    if row['label'] == positive:\n",
    "        if row['label'] == pred:\n",
    "            true_positives.append(idx[0])\n",
    "        else:\n",
    "            false_positives.append(idx[0])\n",
    "    else:\n",
    "        if row['label'] == pred:\n",
    "            true_negatives.append(idx[0])            \n",
    "        else:\n",
    "            false_negatives.append(idx[0])\n",
    "            \n",
    "print(len(true_positives))\n",
    "print(len(false_positives))\n",
    "print(len(true_negatives))\n",
    "print(len(false_negatives))\n",
    "assert len(true_positives) + len(false_positives) + len(true_negatives) + len(false_negatives) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It will choose misclassified example from one of the classes \n",
    "\n",
    "# for idx, (true, pred) in enumerate(zip(test_labels['label'], preds)):\n",
    "#     if true != pred:\n",
    "#         x_test_idx = test_labels.iloc[idx].name[0]\n",
    "#         true_label = true\n",
    "#         x_test = test_timeseries.loc[[x_test_idx], :, :]\n",
    "#         for idx, (true, pred) in enumerate(zip(test_labels['label'], preds)):\n",
    "#             if true == true_label and pred == true:\n",
    "#                 distractor_idx = test_labels.iloc[idx].name[0]\n",
    "#                 distractor = test_timeseries.loc[[distractor_idx], :, :]\n",
    "#                 break\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_method = explainers.OptimizedSearch(pipeline, timeseries, labels, silent=False)\n",
    "\n",
    "explainer_list = {\n",
    "    'bruteforce': explainers.BruteForceSearch(pipeline, timeseries, labels, silent=False, dont_stop=False),\n",
    "    'lime': explainers.LimeExplanation(pipeline, timeseries, labels),\n",
    "    'random': explainers.RandomExplanation(pipeline, timeseries, labels),\n",
    "    'shap': explainers.ShapExplanation(pipeline, timeseries, labels)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TEST ########################\n",
    "\n",
    "ground_truth = set(all_metrics)\n",
    "scores = []\n",
    "\n",
    "no_exp_list_fp = []\n",
    "\n",
    "#TODO: Update here\n",
    "for run in tqdm(random.sample(true_positives,2)):\n",
    "    \n",
    "    x_test = test_timeseries.loc[[run], :, :]\n",
    "    explanation = our_method.explain(x_test)\n",
    "    print(\"OUR EXP: \", explanation)\n",
    "    \n",
    "    #If we cannot find an explanation or found explanation is not in the ground truth\n",
    "    if explanation is None or len(ground_truth.intersection(explanation)) != len(explanation):\n",
    "        no_exp_list_fp.append(run)\n",
    "        target_length = 3 #if our method cannot find an explanation, set target to max number of features\n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': 0,\n",
    "            'precision': 0\n",
    "        })\n",
    "\n",
    "    else:    \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': len(ground_truth.intersection(explanation)) / len(ground_truth),\n",
    "            'precision': len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        })\n",
    "        target_length = len(explanation)\n",
    "    \n",
    "    for e in explainer_list:\n",
    "        print(target_length)\n",
    "        explanation = explainer_list[e].explain(x_test, num_features=target_length)\n",
    "        print(\"SHAP EXP:\",explanation)\n",
    "        \n",
    "        try:\n",
    "            recall = len(ground_truth.intersection(explanation)) / len(ground_truth)\n",
    "            precision = len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        except:\n",
    "            recall = 0\n",
    "            precision = 0\n",
    "        \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': e,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_temp = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=results_temp, x='method', y='recall', kind='bar')\n",
    "plt.ylim(0, 1)\n",
    "sns.catplot(data=results_temp, x='method', y='precision', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set(all_metrics)\n",
    "scores = []\n",
    "\n",
    "no_exp_list_fp = []\n",
    "\n",
    "#TODO: Update here\n",
    "for run in tqdm(false_positives):\n",
    "    \n",
    "    x_test = test_timeseries.loc[[run], :, :]\n",
    "    explanation = our_method.explain(x_test)\n",
    "    \n",
    "    #If we cannot find an explanation or found explanation is not in the ground truth\n",
    "    if explanation is None or len(ground_truth.intersection(explanation)) != len(explanation):\n",
    "        no_exp_list_fp.append(run)\n",
    "        target_length = 3 #if our method cannot find an explanation, set target to max number of features\n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': 0,\n",
    "            'precision': 0\n",
    "        })\n",
    "\n",
    "    else:    \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': len(ground_truth.intersection(explanation)) / len(ground_truth),\n",
    "            'precision': len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        })\n",
    "        target_length = len(explanation)\n",
    "    \n",
    "    for e in explainer_list:\n",
    "        \n",
    "        explanation = explainer_list[e].explain(x_test, num_features=target_length)\n",
    "        \n",
    "        try:\n",
    "            recall = len(ground_truth.intersection(explanation)) / len(ground_truth)\n",
    "            precision = len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        except:\n",
    "            recall = 0\n",
    "            precision = 0\n",
    "        \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': e,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['method'] == 'shap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='recall', kind='bar')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='precision', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set(all_metrics)\n",
    "scores = []\n",
    "\n",
    "no_exp_list_tp = []\n",
    "\n",
    "#TODO: Update here\n",
    "for run in tqdm(true_positives):\n",
    "    \n",
    "    x_test = test_timeseries.loc[[run], :, :]\n",
    "    explanation = our_method.explain(x_test)\n",
    "    \n",
    "    #If we cannot find an explanation or found explanation is not in the ground truth\n",
    "    if explanation is None or len(ground_truth.intersection(explanation)) != len(explanation):\n",
    "        no_exp_list_tp.append(run)\n",
    "        target_length = 3 #if our method cannot find an explanation, set target to max number of features\n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': 0,\n",
    "            'precision': 0\n",
    "        })\n",
    "\n",
    "    else:    \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': len(ground_truth.intersection(explanation)) / len(ground_truth),\n",
    "            'precision': len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        })\n",
    "        target_length = len(explanation)\n",
    "    \n",
    "    for e in explainer_list:\n",
    "        \n",
    "        explanation = explainer_list[e].explain(x_test, num_features=target_length)\n",
    "        \n",
    "        try:\n",
    "            recall = len(ground_truth.intersection(explanation)) / len(ground_truth)\n",
    "            precision = len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        except:\n",
    "            recall = 0\n",
    "            precision = 0\n",
    "        \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': e,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='recall', kind='bar')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='precision', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set(all_metrics)\n",
    "scores = []\n",
    "\n",
    "no_exp_list_fn = []\n",
    "\n",
    "#TODO: Update here\n",
    "for run in tqdm(false_negatives):\n",
    "    \n",
    "    x_test = test_timeseries.loc[[run], :, :]\n",
    "    explanation = our_method.explain(x_test)\n",
    "    \n",
    "    #If we cannot find an explanation or found explanation is not in the ground truth\n",
    "    if explanation is None or len(ground_truth.intersection(explanation)) != len(explanation):\n",
    "        no_exp_list_fn.append(run)\n",
    "        target_length = 3 #if our method cannot find an explanation, set target to max number of features\n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': 0,\n",
    "            'precision': 0\n",
    "        })\n",
    "\n",
    "    else:    \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': 'our_method',\n",
    "            'recall': len(ground_truth.intersection(explanation)) / len(ground_truth),\n",
    "            'precision': len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        })\n",
    "        target_length = len(explanation)\n",
    "    \n",
    "    for e in explainer_list:\n",
    "        \n",
    "        explanation = explainer_list[e].explain(x_test, num_features=target_length)\n",
    "        \n",
    "        try:\n",
    "            recall = len(ground_truth.intersection(explanation)) / len(ground_truth)\n",
    "            precision = len(ground_truth.intersection(explanation)) / len(explanation)\n",
    "        except:\n",
    "            recall = 0\n",
    "            precision = 0\n",
    "        \n",
    "        scores.append({\n",
    "            'run': run,\n",
    "            'method': e,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='recall', kind='bar')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist FP Data\n",
    "sns.catplot(data=results, x='method', y='precision', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist Data\n",
    "sns.catplot(data=results, x='method', y='recall', kind='bar')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for Taxonomist Data\n",
    "sns.catplot(data=results, x='method', y='precision', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
