{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160b4bb4-3f3f-4420-a1c2-24ea1640faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf55d01-f108-499a-991f-87fd7f55ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 22:48:24.238234: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-02 22:48:24.967649: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-02 22:48:25.153988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738554505.602389  362527 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738554505.656808  362527 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-02 22:48:26.315739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_NUM_INTRAOP_THREADS is 1\n",
      "TF_NUM_INTEROP_THREADS is 3\n"
     ]
    }
   ],
   "source": [
    "# third party modules\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n",
    "                                        CSVLogger, EarlyStopping)\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# project modules\n",
    "import datasets as datasets\n",
    "\n",
    "\n",
    "# define environmental variables to prevent overuse of CPU Cores\n",
    "# Access and modify environmental variables\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1' #set to 1\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '3' #set to 1 less than # of requested cores\n",
    "print(f\"TF_NUM_INTRAOP_THREADS is {os.getenv('TF_NUM_INTRAOP_THREADS')}\")\n",
    "print(f\"TF_NUM_INTEROP_THREADS is {os.getenv('TF_NUM_INTEROP_THREADS')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9db06-380e-416f-8cd3-2722eb7fb985",
   "metadata": {},
   "source": [
    "Process Raw Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c809a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tracings data has 20001 samples\n",
      "Sample Shape for tracings: (4096, 12)\n",
      "The exam_id data has 20001 samples\n",
      "Sample Shape for exam_id: ()\n",
      "HDF5 Datasets Combined\n"
     ]
    }
   ],
   "source": [
    "# Create testing and validation set. This is done by combining hdf5 files from the CODE 15% dataset\n",
    "\n",
    "# define base paths\n",
    "base_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/exams_part\"\n",
    "combined_hdf5_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/combined.hdf5\"\n",
    "# user define how many of the 18 files total to incorporate\n",
    "num_files = 1         \n",
    "\n",
    "# use a loop to create the list of strings for each hdf5 part\n",
    "hdf5_paths = []\n",
    "for i in range(num_files): # range(17) = 0 --> 16\n",
    "    hdf5_paths.append(f\"{base_path}{i}.hdf5\")\n",
    "\n",
    "# define dataset names for within each hdf5\n",
    "hdf5_datasets = ['tracings', 'exam_id']\n",
    "\n",
    "\n",
    "# load dataset and create combined file (create if there is none present)\n",
    "#combined_hdf5 = h5py.File(combined_hdf5_path,'w')\n",
    "with h5py.File(combined_hdf5_path, 'w') as combined_hdf5:\n",
    "    for hdf5_dset in hdf5_datasets: # iterate through each dataset in hdf5 paths\n",
    "        # find total number of samples\n",
    "        total_size = sum(h5py.File(path,'r')[hdf5_dset].shape[0] for path in hdf5_paths)\n",
    "        print(f\"The {hdf5_dset} data has {total_size} samples\")\n",
    "        \n",
    "        # find sample shape\n",
    "        sample_shape = h5py.File(hdf5_paths[0],'r')[hdf5_dset].shape[1:]\n",
    "        print(f\"Sample Shape for {hdf5_dset}: {sample_shape}\")\n",
    "\n",
    "        # create new dataset in the combined HDF5 file\n",
    "        combined_dataset = combined_hdf5.create_dataset(\n",
    "            hdf5_dset, \n",
    "            shape=(total_size,) + sample_shape, \n",
    "            dtype=h5py.File(hdf5_paths[0], 'r')[hdf5_dset].dtype)\n",
    "\n",
    "        # copy data from file into the combined dataset\n",
    "        start_idx = 0\n",
    "        for path in hdf5_paths:\n",
    "            with h5py.File(path,'r') as hdf5_file:\n",
    "                data = hdf5_file[hdf5_dset][:]\n",
    "                combined_dataset[start_idx:start_idx + data.shape[0]] = data\n",
    "                start_idx +=data.shape[0]\n",
    "\n",
    "print(\"HDF5 Datasets Combined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0671e9d4-460f-4ccd-addf-dfccff201455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of exam_id is: 20001\n",
      "\n",
      "The size of tracings is: (20001, 4096, 12)\n",
      "\n",
      "\n",
      "The length of the raw CSV is: 345779\n",
      "\n",
      "\n",
      "Row numbers with left_only:\n",
      " [0]\n",
      "\n",
      "columns of training ground truth is Index(['1dAVb', 'RBBB', 'LBBB', 'SB', 'AF', 'ST'], dtype='object')\n",
      "\n",
      "shape of training ground truth is (20000, 6)\n",
      "\n",
      "first 100 rows of training ground truth is\n",
      "       1dAVb  RBBB  LBBB   SB   AF   ST\n",
      "21      0.0   0.0   0.0  0.0  0.0  0.0\n",
      "22      0.0   0.0   0.0  0.0  0.0  0.0\n",
      "30      0.0   0.0   0.0  0.0  0.0  0.0\n",
      "46      0.0   0.0   0.0  0.0  0.0  0.0\n",
      "58      1.0   0.0   0.0  0.0  0.0  0.0\n",
      "...     ...   ...   ...  ...  ...  ...\n",
      "1520    0.0   0.0   0.0  0.0  0.0  0.0\n",
      "1522    0.0   0.0   0.0  0.0  0.0  0.0\n",
      "1533    0.0   0.0   0.0  0.0  0.0  0.0\n",
      "1538    0.0   0.0   0.0  0.0  0.0  1.0\n",
      "1584    0.0   0.0   0.0  0.0  0.0  0.0\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# make corresponding CSV file and final HDF5 file\n",
    "\n",
    "hdf_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/combined.hdf5\"\n",
    "csv_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/exams15.csv\"\n",
    "modified_csv_save_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/csv_merged.csv\"\n",
    "\n",
    "# read the csv into a pandas dataframe\n",
    "csv_df = pd.read_csv(csv_path)  \n",
    "\n",
    "with h5py.File(hdf_path, 'r') as f:\n",
    "    # define values to sort\n",
    "    dat_exam_id_initial = f['exam_id']                 # creates array of examID's from hdf5\n",
    "    dat_tracings = f['tracings']                # create array of tracings \n",
    "    dat_exam_id = pd.DataFrame(dat_exam_id_initial)   # convert dat_exam_id to a pandas dataframe\n",
    "    dat_exam_id.columns = ['exam_id']          # assign column name\n",
    "\n",
    "    # print properties of initial dataframes\n",
    "    print(f\"The length of exam_id is: {len(dat_exam_id)}\\n\")\n",
    "    print(f\"The size of tracings is: {dat_tracings.shape}\\n\\n\")\n",
    "    print(f\"The length of the raw CSV is: {len(csv_df)}\\n\\n\")\n",
    "\n",
    "    # use outer merge since there may be rows that have no mathces (will remove)\n",
    "    # search for rows in hdf_5 that are not in sorted_csv and remove them\n",
    "    # perform initial outer merge\n",
    "    init_csv = dat_exam_id.merge(csv_df, how='outer', indicator=True) \n",
    "\n",
    "    # find indicies of rows that are \"left_only\" (so they can be removed from hdf5)\n",
    "    # Filter rows where _merge is 'left_only'\n",
    "    left_only_rows = init_csv[init_csv['_merge'] == 'left_only']\n",
    "    # Get the index values of the filtered rows\n",
    "    left_only_row_numbers = left_only_rows.index.tolist()\n",
    "    print(f'Row numbers with left_only:\\n {left_only_row_numbers}\\n')\n",
    "    # Get the index values of rows that are not filtered out\n",
    "    index_nums = [i for i in range(len(dat_tracings)) if i not in left_only_row_numbers]\n",
    "\n",
    "    # drop rows that do not match from csv (left or right)\n",
    "    csv_merged = init_csv[init_csv['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "\n",
    "    # remove and rearrange columns\n",
    "    csv_merged = csv_merged.drop(['exam_id', 'age','is_male','nn_predicted_age','patient_id','death','timey','normal_ecg','trace_file'], axis=1)\n",
    "    csv_merged = csv_merged[['1dAVb','RBBB','LBBB','SB','AF','ST']]\n",
    "    csv_merged = csv_merged.astype(np.float32)\n",
    "    print(f'columns of training ground truth is {csv_merged.columns}\\n')\n",
    "    print(f'shape of training ground truth is {csv_merged.shape}\\n')\n",
    "    print(f'first 100 rows of training ground truth is\\n {csv_merged[:100]}')\n",
    "\n",
    "    # save pandas dataframe to csv\n",
    "    csv_merged.to_csv(modified_csv_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b9269d-ff61-4a7f-9714-6a6fb5562002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping select rows from hdf5 tensors\n",
      "(20000,)\n",
      "(20000, 4096, 12)\n"
     ]
    }
   ],
   "source": [
    "# remove rows of hdf5 tensors that have exam_id's that don't match anything in the CSV\n",
    "print('Keeping select rows from hdf5 tensors')\n",
    "\n",
    "with h5py.File(hdf_path, 'r+') as f:\n",
    "    # define values to sort\n",
    "    vals_to_sort = f['exam_id'][index_nums]                 # creates array of examID's from hdf5\n",
    "    dat_tracings = f['tracings'][index_nums]                # create array of tracings \n",
    "\n",
    "    # print shape to ensure the right amount of elements have been removed\n",
    "    print(vals_to_sort.shape)    \n",
    "    print(dat_tracings.shape)\n",
    "\n",
    "    # delete and create datasets (due to size mismatch)\n",
    "    # Delete the old datasets if they exist\n",
    "    del f['exam_id']\n",
    "    del f['tracings']\n",
    "\n",
    "    # Create new datasets with the filtered data\n",
    "    f.create_dataset('exam_id', data=vals_to_sort)\n",
    "    f.create_dataset('tracings', data=dat_tracings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb35bfa-1ddf-4ea1-826d-5baaa02ebe4d",
   "metadata": {},
   "source": [
    "Run Model on Test Set and Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074d102-e43c-499d-989a-bb895b51a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look into testing set to select test point\n",
    "\n",
    "# load testing dataset \n",
    "path_to_hdf5_test = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE/ecg_tracings.hdf5\"\n",
    "dataset_name_test = \"tracings\"  \n",
    "\n",
    "# Import data. SEQ is an instance of class ECGSequence\n",
    "seq = datasets.ECGSequence(path_to_hdf5_test, dataset_name_test)  # using default batch size\n",
    "\n",
    "# load pretrained model (still need to compile later) \n",
    "model_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/PretrainedModels/model/model.hdf5\"\n",
    "pre_model = load_model(model_path)  \n",
    "\n",
    "\n",
    "# compile and apply model to testing dataset\n",
    "pre_model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "model_predictions = pre_model.predict(seq,verbose=1)   # y_score is a numpy array with dimensions 827x6. It holds the predictions generated by the model\n",
    "\n",
    "# extra\n",
    "print(model_predictions.shape)\n",
    "print(model_predictions[:5])\n",
    "\n",
    "# Generate dataframe\n",
    "np.save(\"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/dnn_output.npy\", model_predictions)\n",
    "print(\"Output predictions saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "# diagnosis order = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'AF', 'ST']\n",
    "# label 0 = 'Normal'\n",
    "# label 1 = '1dAVb'\n",
    "# label 2 = 'RBBB'\n",
    "# ...\n",
    "\n",
    "# apply threshold\n",
    "threshold = np.array([0.124, 0.07, 0.05, 0.278, 0.390, 0.174])\n",
    "\n",
    "# apply threshold to convert array of SELF-GENERATED probabilities to array of selections \n",
    "mask = model_predictions > threshold # record instances in which y_score_best > threshold\n",
    "y_pred_2D = np.zeros_like(model_predictions)         # fill array with same size as y_score_best with zeros\n",
    "y_pred_2D[mask] = 1                                  # set certain values (defined by mask) to 1\n",
    "\n",
    "# true values for test set\n",
    "# load ground truth for test set\n",
    "y_true_2D = pd.read_csv('/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE/annotations/gold_standard.csv').values\n",
    "\n",
    "\n",
    "# process arrays of predictions so lables are numbered (dimension 827x6 --> 827x1)\n",
    "y_pred = []\n",
    "for i in range(y_pred_2D.shape[0]):\n",
    "    one_present = 0\n",
    "    for j in range(y_pred_2D.shape[1]):   # for each row, iterate through columns\n",
    "        if y_pred_2D[i, j] == 1:\n",
    "            y_pred.append(j + 1)\n",
    "            one_present = 1\n",
    "            break\n",
    "    if one_present == 0:   # after each row, check if a 1 has been assigned\n",
    "        y_pred.append(0)\n",
    "        \n",
    "y_true = []\n",
    "for i in range(y_true_2D.shape[0]):\n",
    "    one_present = 0\n",
    "    for j in range(y_true_2D.shape[1]):   # for each row, iterate through columns\n",
    "        if y_true_2D[i, j] == 1:\n",
    "            y_true.append(j + 1)\n",
    "            one_present = 1\n",
    "            break\n",
    "    if one_present == 0:   # after each row, check if a 1 has been assigned\n",
    "        y_true.append(0)\n",
    "\n",
    "\n",
    "# select indices/conditions for CoMTE\n",
    "true_select = 3 #UPDATE HERE FOR OTHER CLASSES\n",
    "pred_select = 1 #UPDATE HERE FOR OTHER CLASSES\n",
    "\n",
    "# find relevant indices\n",
    "indices_test = []\n",
    "for idx, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
    "    print(f\"Index:{idx}, True Label: {true}, Predicted Label: {pred}\") # print elements\n",
    "    if true ==  true_select and pred == pred_select:\n",
    "        indices_test.append(idx)   \n",
    "        \n",
    "print('\\n\\n\\n')\n",
    "print(f\"The {indices_test} indices match the case defined above:\\n(true_select = {true_select}, pred_select = {pred_select})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c74fe-61a3-438b-8ebb-6074f8374032",
   "metadata": {},
   "source": [
    "Apply CoMTE_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee189792-d946-4747-ae22-89734f2bd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1: A Classifier that works with COMLEX\n",
    "\n",
    "The classifier must have 2 capabilities:\n",
    "1. Predict a class ie: class 0 in classes {0, 1}\n",
    "2. Predict the probability for each class\n",
    "-ie: [0.1, 0.9]\n",
    "\n",
    "and\n",
    "\n",
    "Be able to execute capability 1 and 2 on a PANDAS dataframe,\n",
    "returning an array of corresponding predictions.\n",
    "\"\"\"\n",
    "\n",
    "class BasicClassifier:\n",
    "    classifier = pre_model  # tensorflow CNN\n",
    "    import os\n",
    "    \n",
    "    @staticmethod\n",
    "    def contrived_classification(pandas_dfs):\n",
    "        classifier = pre_model  # tensorflow CNN\n",
    "\n",
    "        # convert 2D pandas df to 3D dataframe (N,4096,12)\n",
    "        array_3d = pandas_dfs.to_numpy().reshape(int(pandas_dfs.shape[0]/4096), 4096, 12)\n",
    "\n",
    "        # create instance of ECGSequence to store the (N,4096,12) dataset\n",
    "        temp_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/temporary.hdf5\"\n",
    "        temp_dataset_name = \"tracings\"\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        # create hdf with appropriate data\n",
    "        hdf_file = h5py.File(temp_path, 'w')\n",
    "        hdf_file.create_dataset(temp_dataset_name,data = array_3d)\n",
    "        # init instnace of ECG Sequence holding modified with hdf path\n",
    "        modified_instance = datasets.ECGSequence(temp_path, temp_dataset_name)\n",
    "\n",
    "        # get classification and probability\n",
    "        probability = classifier.predict(modified_instance, verbose = 1)    \n",
    "        \n",
    "    \n",
    "        # close hdf5's\n",
    "        modified_instance._closehdf()\n",
    "        hdf_file.close()\n",
    "        os.remove(temp_path)\n",
    "\n",
    "        # analyze model output with thresholding\n",
    "        # define given thresholds\n",
    "        threshold = np.array([0.124, 0.07, 0.05, 0.278, 0.390, 0.174])\n",
    "        \n",
    "        # generate class 0 probability\n",
    "        exceedances = 1 - (np.maximum((probability - threshold) , 0) / (1 - threshold))\n",
    "        normal_prob = np.mean(exceedances, axis = 1, keepdims = True) # normal prob should be (N,1)\n",
    "        \n",
    "        # Add normal_prob as a new column\n",
    "        probability_n = np.column_stack((normal_prob, probability))     \n",
    "\n",
    "        # new threshold\n",
    "        new_threshold = np.array([1, 0.124, 0.07, 0.05, 0.278, 0.390, 0.174])\n",
    "        \n",
    "        mask = probability_n >= new_threshold\n",
    "        sample_classes = []\n",
    "        \n",
    "        for row, mask in zip(probability_n, mask):\n",
    "            passing_indices = np.where(mask)[0]\n",
    "            if len(passing_indices) > 1:  # If more than one indices pass\n",
    "                # find margin between threshold and probability\n",
    "                diff_array = probability_n - new_threshold\n",
    "                passing_index = np.argmax(diff_array)\n",
    "                # append the index that has the highest margin\n",
    "                sample_classes.append(passing_index)\n",
    "            \n",
    "            elif len(passing_indices) == 0:  # no passes\n",
    "                sample_classes.append(0) \n",
    "            else:\n",
    "                sample_classes.append(passing_indices[0])  # Select the first (or adjust logic)\n",
    "                \n",
    "        return sample_classes\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def contrived_classification_proba(pandas_dfs):\n",
    "        classifier = pre_model  # tensorflow CNN\n",
    "        \n",
    "        # convert 2D pandas df to 3D dataframe (N,4096,12)\n",
    "        array_3d = pandas_dfs.to_numpy().reshape(int(pandas_dfs.shape[0]/4096), 4096, 12)\n",
    "\n",
    "        # create instance of ECGSequence to store the (N,4096,12) dataset\n",
    "        temp_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/temporary.hdf5\"\n",
    "        temp_dataset_name = \"tracings\"\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        # create hdf with appropriate data\n",
    "        hdf_file = h5py.File(temp_path, 'w')\n",
    "        hdf_file.create_dataset(temp_dataset_name,data = array_3d)\n",
    "        # init instnace of ECG Sequence holding modified with hdf path\n",
    "        modified_instance = datasets.ECGSequence(temp_path, temp_dataset_name)\n",
    "\n",
    "        # get classification and probability\n",
    "        probability = classifier.predict(modified_instance, verbose = 0)  \n",
    "        \n",
    "    \n",
    "        # close hdf5's\n",
    "        modified_instance._closehdf()\n",
    "        hdf_file.close()\n",
    "        os.remove(temp_path)\n",
    "\n",
    "        \n",
    "        # analyze model output with thresholding\n",
    "         # define given thresholds\n",
    "        threshold = np.array([0.124, 0.07, 0.05, 0.278, 0.390, 0.174])\n",
    "        \n",
    "        # generate class 0 probability\n",
    "        exceedances = 1 - (np.maximum((probability - threshold) , 0) / (1 - threshold))\n",
    "        normal_prob = np.mean(exceedances)\n",
    "\n",
    "        # modify result \n",
    "        probability = np.insert(probability,0,normal_prob)   \n",
    "\n",
    "        # probability should be in a 2D array format\n",
    "        if probability.ndim == 1:  # Check if it's 1D\n",
    "            probability = probability.reshape(1, -1)\n",
    "        \n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59684396-ae9c-4814-81a1-1afcb93aaed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in the dataset is 20000\n",
      "The index in which the validation set starts and train set ends is 19600\n",
      "<datasets.ECGSequence object at 0x147267f51660>\n",
      "<datasets.ECGSequence object at 0x147267f53d60>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 2: Training data and labels\n",
    "\n",
    "[The explanation will use counterfactuals drawn from this input data]\n",
    "\n",
    "The training data can be should be an iterable of samples\n",
    "(ie: python array, numpy array, pandas dataframe),\n",
    "where each sample needs to be the same size array as the others.\n",
    "\n",
    "The labels should be a corresponding iterable to the samples.\n",
    "\n",
    "COMLEX will only use samples for which the labels are the same\n",
    "as the prediction from the trained classifier.\n",
    "\n",
    "Note:\n",
    "We don't support variable-length training data at this time,\n",
    "use a different projection of the data if you have such data.\n",
    "\"\"\"\n",
    "\n",
    "class BasicData:\n",
    "    # define basic variables\n",
    "    classes_available = [0,1,2,3,4,5,6]\n",
    "    num_columns = 4096\n",
    "\n",
    "    # define key paths and variables for training data\n",
    "    path_to_hdf5_test = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE/ecg_tracings.hdf5\"\n",
    "    num_features = 12\n",
    "    dataset_name_hdf_tracings = \"tracings\" \n",
    "    training_set_hdf_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/combined.hdf5\"\n",
    "    y_train_csv_path = \"/projectnb/peaclab-mon/JLi/projectx/AutoECGDiagnosisData/CODE15/csv_merged.csv\"\n",
    "    # read the csv into a np dataframe \n",
    "    np_train_labels = np.genfromtxt(y_train_csv_path, delimiter=\",\")\n",
    "    \n",
    "    # for the ECG implementation, the data wrapper must convert a 3D HDF5 file into a pandas multiindex array\n",
    "    # create instances of ECGSequence for train data \n",
    "    train_seq, valid_seq = datasets.ECGSequence.get_train_and_val(training_set_hdf_path, dataset_name_hdf_tracings, y_train_csv_path,val_split=0.02)\n",
    "    # return array-like samples for the data wrapper (returns 20000x4096x12 np array)\n",
    "    timeseries = train_seq._gettimeseries_()\n",
    "    num_features = 12\n",
    "    # iterable of corresponding labels for the samples for the data wrapper (returns 20000x6 np array) <--- take out first column that represents ExamID\n",
    "    labels = train_seq._gettruelabel_()[:,1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82133f73-0c8d-46f1-9bfb-c8a49a79fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 3: Wrapping it up.\n",
    "\n",
    "The training data, training labels, and trained classifier need to be wrapped up\n",
    "into a form that can pass through COMLEX.\n",
    "\n",
    "While wrapping up the training data and labels is relatively straightforward,\n",
    "wrapping up the classifier is more difficult\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/projectnb/peaclab-mon/JLi/projectx/CoMTE_V2_JLi/comlex_core')  # Path to the comlex_core directory\n",
    "\n",
    "# import project (wrapper) modules\n",
    "from src import explainers\n",
    "from src.explainable_model_ECG import ClfModel as ClfModel\n",
    "from src.explainable_data_ECG import ClfData as ClfData\n",
    "\n",
    "class BasicComlexInput:\n",
    "\n",
    "    # 1. wrap training points\n",
    "    df_train_points = ClfData.wrap_df_x(BasicData.timeseries, BasicData.num_features)\n",
    "    \n",
    "    # 2. wrap training labels\n",
    "    df_train_labels = ClfData.wrap_df_y(BasicData.labels)\n",
    "    \n",
    "    # 3. wrap up the classifier\n",
    "    # note: column_attr, or the corresponding name of the columns in the sample,\n",
    "    #  is unique to dataframes, and auto-generated by wrap_df_x\n",
    "    wrapped_classifier = ClfModel(BasicClassifier.classifier,\n",
    "                                predict_attr=BasicClassifier.contrived_classification,\n",
    "                                predict_proba_attr=BasicClassifier.contrived_classification_proba,\n",
    "                                column_attr=df_train_points.columns.values.tolist(),\n",
    "                                classes_attr=BasicData.classes_available,\n",
    "                                window_size_attr=BasicData.num_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ec6220-991d-4779-a4b4-0c447c5fd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: run through COMLEX\n",
    "\n",
    "\"\"\"\n",
    "Part 4: Running it through COMLEX\n",
    "\n",
    "Requires:\n",
    "1. wrapped classifier\n",
    "2. wrapped training data\n",
    "3. wrapped training labels\n",
    "\n",
    "To run COMLEX:\n",
    "1. wrap the test point\n",
    "2. instantiate a comlex runner on the wrapped components\n",
    "-OptimizedSearch sets up a KDTree for based on the data,\n",
    " in order to speed up the search time for the counterfactual\n",
    " explanation.\n",
    "-OptimizedSearch will fallback to BruteForceSearch if it fails\n",
    " to find a counterfactual explanation with a predicted\n",
    " probability greater than 0.95.\n",
    "3. use the comlex runner to explain wrapped datapoint\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# get testing point\n",
    "test_point = seq._getsample_(253)\n",
    "# wrap test point \n",
    "test_df = ClfData.wrap_df_test_point(test_point)\n",
    "\n",
    "# 2. set up an optimized search comlex runner\n",
    "comlex = explainers.OptimizedSearch(BasicComlexInput.wrapped_classifier,\n",
    "                                    BasicComlexInput.df_train_points,\n",
    "                                    BasicComlexInput.df_train_labels,\n",
    "                                    silent=True, threads=4, num_distractors=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc410ae-14a9-4d1a-8bfc-72805ca8a562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "[[9.03648454e-01 1.70873955e-01 2.50145164e-03 5.48370183e-01\n",
      "  1.71687198e-03 2.25917041e-01 8.82774126e-04]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09635155  0.04687395 -0.06749855  0.49837018 -0.27628313 -0.16408296\n",
      "  -0.17311723]]\n",
      "3\n",
      "using greedy search\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "[[9.03648454e-01 1.70873955e-01 2.50145164e-03 5.48370183e-01\n",
      "  1.71687198e-03 2.25917041e-01 8.82774126e-04]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09635155  0.04687395 -0.06749855  0.49837018 -0.27628313 -0.16408296\n",
      "  -0.17311723]]\n",
      "3\n",
      "trying distractor 1 of 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[[9.03648454e-01 1.70873955e-01 2.50145164e-03 5.48370183e-01\n",
      "  1.71687198e-03 2.25917041e-01 8.82774126e-04]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09635155  0.04687395 -0.06749855  0.49837018 -0.27628313 -0.16408296\n",
      "  -0.17311723]]\n",
      "3\n",
      "current probas is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "label is [np.int64(3)]\n",
      "to max is 1\n",
      "finding best col\n",
      "('DI', 1)\n",
      "0.077106886\n",
      "('DII', 1)\n",
      "0.038080294\n",
      "('DIII', 1)\n",
      "0.03133081\n",
      "('AVR', 1)\n",
      "0.08052611\n",
      "('AVL', 1)\n",
      "0.09074222\n",
      "('AVF', 1)\n",
      "0.09611185\n",
      "('V1', 1)\n",
      "0.040726554\n",
      "('V2', 1)\n",
      "0.07599588\n",
      "('V3', 1)\n",
      "0.06288811\n",
      "('V4', 1)\n",
      "0.06918588\n",
      "('V5', 1)\n",
      "0.065757275\n",
      "('V6', 1)\n",
      "0.089020945\n",
      "checking for beats\n",
      "distractor is [[9.9826193e-01 1.3313544e-01 1.0480490e-04 3.5902809e-05 5.4407836e-04\n",
      "  8.9224763e-05 5.1648698e-07]]\n",
      "prediction is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "best case is 0.1708739548921585\n",
      "best column is None\n",
      "trying distractor 2 of 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[[9.03648454e-01 1.70873955e-01 2.50145164e-03 5.48370183e-01\n",
      "  1.71687198e-03 2.25917041e-01 8.82774126e-04]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09635155  0.04687395 -0.06749855  0.49837018 -0.27628313 -0.16408296\n",
      "  -0.17311723]]\n",
      "3\n",
      "current probas is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "label is [np.int64(3)]\n",
      "to max is 1\n",
      "finding best col\n",
      "('DI', 1)\n",
      "0.13332534\n",
      "('DII', 1)\n",
      "0.0671844\n",
      "('DIII', 1)\n",
      "0.07766187\n",
      "('AVR', 1)\n",
      "0.11935504\n",
      "('AVL', 1)\n",
      "0.17423548\n",
      "('AVF', 1)\n",
      "0.12279555\n",
      "('V1', 1)\n",
      "0.03193714\n",
      "('V2', 1)\n",
      "0.1023686\n",
      "('V3', 1)\n",
      "0.10757274\n",
      "('V4', 1)\n",
      "0.11225135\n",
      "('V5', 1)\n",
      "0.11272897\n",
      "('V6', 1)\n",
      "0.15019765\n",
      "checking for beats\n",
      "distractor is [[8.9140248e-01 6.9478852e-01 2.4340152e-04 2.4297186e-04 5.6118110e-06\n",
      "  1.4991580e-03 6.3927779e-08]]\n",
      "prediction is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "best case is 0.1708739548921585\n",
      "best column is AVL\n",
      "new explanation is ['AVL']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[[0.91117839 0.17423548 0.00318589 0.50180405 0.00105872 0.22352172\n",
      "  0.0021185 ]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.08882161  0.05023548 -0.06681411  0.45180405 -0.27694128 -0.16647828\n",
      "  -0.1718815 ]]\n",
      "3\n",
      "current probas is [[0.9111784  0.17423548 0.00318589 0.50180405 0.00105872 0.22352172\n",
      "  0.0021185 ]]\n",
      "label is [np.int64(3)]\n",
      "to max is 1\n",
      "finding best col\n",
      "('DI', 1)\n",
      "0.092577465\n",
      "('DII', 1)\n",
      "0.102935344\n",
      "('DIII', 1)\n",
      "0.0669254\n",
      "('AVR', 1)\n",
      "0.18193877\n",
      "('AVF', 1)\n",
      "0.13998269\n",
      "('V1', 1)\n",
      "0.0579398\n",
      "('V2', 1)\n",
      "0.08310421\n",
      "('V3', 1)\n",
      "0.08732451\n",
      "('V4', 1)\n",
      "0.11835465\n",
      "('V5', 1)\n",
      "0.12704508\n",
      "('V6', 1)\n",
      "0.1493807\n",
      "checking for beats\n",
      "distractor is [[8.9140248e-01 6.9478852e-01 2.4340152e-04 2.4297186e-04 5.6118110e-06\n",
      "  1.4991580e-03 6.3927779e-08]]\n",
      "prediction is [[0.9111784  0.17423548 0.00318589 0.50180405 0.00105872 0.22352172\n",
      "  0.0021185 ]]\n",
      "best case is 0.1742354780435562\n",
      "best column is AVR\n",
      "new explanation is ['AVL', 'AVR']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[[9.05570878e-01 1.81938767e-01 1.80385832e-03 5.25412858e-01\n",
      "  4.99242800e-04 1.73069134e-01 1.46975706e-03]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09442912  0.05793877 -0.06819614  0.47541286 -0.27750076 -0.21693087\n",
      "  -0.17253024]]\n",
      "3\n",
      "current probas is [[9.0557086e-01 1.8193877e-01 1.8038583e-03 5.2541286e-01 4.9924280e-04\n",
      "  1.7306913e-01 1.4697571e-03]]\n",
      "label is [np.int64(3)]\n",
      "to max is 1\n",
      "finding best col\n",
      "('DI', 1)\n",
      "0.08255842\n",
      "('DII', 1)\n",
      "0.09438761\n",
      "('DIII', 1)\n",
      "0.07745015\n",
      "('AVF', 1)\n",
      "0.15808988\n",
      "('V1', 1)\n",
      "0.07519308\n",
      "('V2', 1)\n",
      "0.083269484\n",
      "('V3', 1)\n",
      "0.08234695\n",
      "('V4', 1)\n",
      "0.11435737\n",
      "('V5', 1)\n",
      "0.119325146\n",
      "('V6', 1)\n",
      "0.13683577\n",
      "checking for beats\n",
      "distractor is [[8.9140248e-01 6.9478852e-01 2.4340152e-04 2.4297186e-04 5.6118110e-06\n",
      "  1.4991580e-03 6.3927779e-08]]\n",
      "prediction is [[9.0557086e-01 1.8193877e-01 1.8038583e-03 5.2541286e-01 4.9924280e-04\n",
      "  1.7306913e-01 1.4697571e-03]]\n",
      "best case is 0.1819387674331665\n",
      "best column is None\n",
      "trying distractor 3 of 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[[9.03648454e-01 1.70873955e-01 2.50145164e-03 5.48370183e-01\n",
      "  1.71687198e-03 2.25917041e-01 8.82774126e-04]]\n",
      "[1.    0.124 0.07  0.05  0.278 0.39  0.174]\n",
      "[[-0.09635155  0.04687395 -0.06749855  0.49837018 -0.27628313 -0.16408296\n",
      "  -0.17311723]]\n",
      "3\n",
      "current probas is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "label is [np.int64(3)]\n",
      "to max is 1\n",
      "finding best col\n",
      "('DI', 1)\n",
      "0.10675046\n",
      "('DII', 1)\n",
      "0.076599635\n",
      "('DIII', 1)\n",
      "0.079660065\n",
      "('AVR', 1)\n",
      "0.095755145\n",
      "('AVL', 1)\n",
      "0.17018145\n",
      "('AVF', 1)\n",
      "0.13681524\n",
      "('V1', 1)\n",
      "0.07935617\n",
      "('V2', 1)\n",
      "0.1097392\n",
      "('V3', 1)\n",
      "0.05943393\n",
      "('V4', 1)\n",
      "0.094777845\n",
      "('V5', 1)\n",
      "0.10142316\n",
      "('V6', 1)\n",
      "0.12791516\n",
      "checking for beats\n",
      "distractor is [[9.4643015e-01 4.0556329e-01 2.0742632e-04 1.1700221e-04 6.9381960e-05\n",
      "  2.7975693e-04 1.1152249e-06]]\n",
      "prediction is [[9.0364844e-01 1.7087395e-01 2.5014516e-03 5.4837018e-01 1.7168720e-03\n",
      "  2.2591704e-01 8.8277413e-04]]\n",
      "best case is 0.1708739548921585\n",
      "best column is None\n"
     ]
    }
   ],
   "source": [
    "# 3. explain the test point\n",
    "# make sure target_class != test_df_class, or else comlex.explain does nothing\n",
    "# test_df_class = contrived_classification(test_df) # = 0\n",
    "target_class = 1\n",
    "explanation = comlex.explain(test_df,to_maximize=target_class,\n",
    "                             return_dist=True,single=True,\n",
    "                             savefig=True,train_iter=10,\n",
    "                             timeseries=False,filename=\"sample_result.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b7e4d3-f61e-4056-8f6e-a99adf2ac402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation is (set(),                   DI  DII  DIII  AVR  AVL  AVF   V1   V2   V3   V4   V5   V6\n",
      "index timestamp                                                             \n",
      "369   0          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      1          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      2          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      3          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...              ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "      4091       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4092       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4093       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4094       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4095       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[4096 rows x 12 columns])\n",
      "\n",
      "The classification of [[[-1.2930014   1.31576277  2.60876415 ...  0.2671492  -0.24547439\n",
      "    0.20240709]\n",
      "  [-1.24199032  1.32614384  2.56813418 ...  0.23937853 -0.25027976\n",
      "    0.1706851 ]\n",
      "  [-1.1966597   1.28050204  2.47716181 ...  0.2286611  -0.24390517\n",
      "    0.16006275]\n",
      "  ...\n",
      "  [-1.12974004 -0.02238396  1.10735609 ... -0.06461882 -0.21788382\n",
      "   -0.85830184]\n",
      "  [-1.06708513 -0.0838424   0.98324271 ... -0.03048815 -0.18292888\n",
      "   -0.79269181]\n",
      "  [-1.03411749 -0.05886769  0.97524978 ... -0.02029714 -0.18191268\n",
      "   -0.775797  ]]]\n",
      "can be changed to 1\n",
      "by changing the sample at points set(),\n",
      "with points from the distractor:\n",
      "                  DI  DII  DIII  AVR  AVL  AVF   V1   V2   V3   V4   V5   V6\n",
      "index timestamp                                                             \n",
      "369   0          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      1          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      2          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      3          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4          0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...              ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "      4091       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4092       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4093       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4094       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "      4095       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[4096 rows x 12 columns]\n",
      "\n",
      "The modified sample that would lead to a different classification is:\n",
      "            DI       DII      DIII       AVR       AVL       AVF        V1  \\\n",
      "0    -1.293001  1.315763  2.608764 -1.954402  1.961154 -0.014868  0.746811   \n",
      "1    -1.241990  1.326144  2.568134 -1.903523  1.942506 -0.046326  0.688564   \n",
      "2    -1.196660  1.280502  2.477162 -1.836911  1.875021 -0.045732  0.655495   \n",
      "3    -1.270278  1.303682  2.573960 -1.926261  1.935427 -0.016992  0.733114   \n",
      "4    -1.267785  1.255061  2.522846 -1.896610  1.884554  0.004741  0.752425   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4091 -1.056380  0.009373  1.065753 -1.061630  0.533897  0.519487 -0.209536   \n",
      "4092 -1.068146  0.021606  1.089752 -1.078503  0.551126  0.519859 -0.200602   \n",
      "4093 -1.129740 -0.022384  1.107356 -1.118399  0.539932  0.571826 -0.180566   \n",
      "4094 -1.067085 -0.083842  0.983243 -1.021353  0.449700  0.571653 -0.144819   \n",
      "4095 -1.034117 -0.058868  0.975250 -0.999368  0.454904  0.546035 -0.143574   \n",
      "\n",
      "            V2        V3        V4        V5        V6  \n",
      "0     0.330725  0.350488  0.267149 -0.245474  0.202407  \n",
      "1     0.293995  0.311442  0.239379 -0.250280  0.170685  \n",
      "2     0.266771  0.297259  0.228661 -0.243905  0.160063  \n",
      "3     0.323062  0.345487  0.274803 -0.234920  0.194797  \n",
      "4     0.354076  0.359057  0.298752 -0.211294  0.213125  \n",
      "...        ...       ...       ...       ...       ...  \n",
      "4091  0.020668  1.485185 -0.111832 -0.245251 -0.841509  \n",
      "4092  0.037837  1.534312 -0.094164 -0.240169 -0.839409  \n",
      "4093  0.071370  1.672116 -0.064619 -0.217884 -0.858302  \n",
      "4094  0.083842  1.593006 -0.030488 -0.182929 -0.792692  \n",
      "4095  0.077508  1.558969 -0.020297 -0.181913 -0.775797  \n",
      "\n",
      "[4096 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# analyze output\n",
    "print(f\"explanation is {explanation}\\n\")\n",
    "\n",
    "replacements_np = explanation[0]\n",
    "replacements = {str(item) for item in replacements_np}\n",
    "distractor_new = explanation[1]\n",
    "counterfactual_explanation = test_df.copy()\n",
    "#counterfactual_explanation = [point for point in test_df] # make copy of original test data before doing replacements\n",
    "\n",
    "for replacement_i in replacements:\n",
    "    counterfactual_explanation[replacement_i] = distractor_new[replacement_i].values[0]\n",
    "\n",
    "print(f\"The classification of {test_point}\\n\"\n",
    "      f\"can be changed to {target_class}\\n\" +\n",
    "      f\"by changing the sample at points {explanation[0]},\\n\" +\n",
    "      f\"with points from the distractor:\\n{explanation[1]}\\n\\n\" +\n",
    "      f\"The modified sample that would lead to a different classification is:\\n{counterfactual_explanation}\")\n",
    "\n",
    "#print(BasicComlexInput.df_train_points.columns.values.tolist())\n",
    "#print(BasicData.classes_available)\n",
    "#print(BasicData.num_columns)\n",
    "#print(BasicComlexInput.df_train_points)\n",
    "#print(\"\")\n",
    "#print(BasicComlexInput.df_train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b52327-b073-4f47-87e6-78c0440c9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
